{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-shanghai",
   "metadata": {},
   "source": [
    "### DCGAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-courtesy",
   "metadata": {},
   "source": [
    "* These are one of the successful GANs as it makes use of convolutional layers\n",
    "* Downsampling and upsampling is performed by convolutional stride and transposed convolution\n",
    "* It doesn't make use of fully conected layers\n",
    "* The trained discriminators are used for image classification tasks, showing competitive performance with other unsupervised algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-beach",
   "metadata": {},
   "source": [
    "The DCGAN architecture can be shown as below\n",
    "<img src=\"../../images/dcgan.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cognitive-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torchvision.datasets as datasets\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepted-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # N xf_g*16 x 4 x 4\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # 8x8\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1),  # 16x16 \n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1),  # 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g*2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.Tanh(),  #[-1,1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            bias= False),\n",
    "            nn.BatchNorm2d(out_channels), # when we use the batch_norm bias is not used\n",
    "            nn.ReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attractive-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "        # Input: N x channels_img x 64 64    \n",
    "        nn.Conv2d(\n",
    "            channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "        ),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        self._block(features_d, features_d*2, 4, 2, 1),\n",
    "        self._block(features_d*2, features_d*4, 4, 2, 1),\n",
    "        self._block(features_d*4, features_d*8, 4, 2, 1),\n",
    "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), #1x1\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            bias= False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE=128\n",
    "IMAGE_SIZE=64\n",
    "CHANNELS_IMG = 1\n",
    "NUM_EPOCHS = 2\n",
    "NOISE_DIM=100\n",
    "Z_DIM=100\n",
    "FEATURES_DISC=64\n",
    "FEATURES_GEN=64\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        )\n",
    "    \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =datasets.MNIST(root='../data', train=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "placed-spiritual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader= DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.rand(32, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "\n",
    "step = 8\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "temporal-westminster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/2] Batch 0/469                       Loss D: 0.6888, loss G: 0.7820\n",
      "Epoch [1/2] Batch 0/469                       Loss D: 0.4925, loss G: 1.3469\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real,_) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "        fake = gen(noise)\n",
    "        ### Train discriminator max log((D(x))) + lo(1-D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1) #N\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        ### Train Generator min log(1-D(G(z))) <-> max log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                #data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "            step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-silver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-coral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
